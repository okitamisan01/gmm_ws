import os
import random
import numpy as np
import IPython.display as ipd
import matplotlib.pyplot as plt
import seaborn as sns

# --- å‰æå¤‰æ•°ï¼ˆã“ã‚Œã‚‰ã¯ã™ã§ã«å¾—ã¦ã„ã‚‹ã‚‚ã®ã¨ä»®å®šï¼‰
# dataset_test: pandas DataFrameï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿, imgãƒ‘ã‚¹, sndãƒ‘ã‚¹, textæ¬„ãªã©ã‚’å«ã‚€ï¼‰
# gmm_preds: å„ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿æ‰€å±ãƒ©ãƒ™ãƒ«ï¼ˆé…åˆ—é•· Nï¼‰
# n_clusters = gmm.n_components

import os
import random
import numpy as np
import pandas as pd
import librosa
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, silhouette_score, silhouette_samples
from sklearn.manifold import TSNE
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import *
from tensorflow.keras.applications import Xception
from tensorflow.keras import Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import categorical_crossentropy

# ====================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ====================
# ç¾åœ¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸€ã¤ä¸Šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

FULL = True
if FULL:
    dataset_train = pd.read_csv(os.path.join(base_dir, "dataset_full", "dataset_train_full.csv"), index_col=0)
    dataset_test  = pd.read_csv(os.path.join(base_dir, "dataset_full", "dataset_test_full.csv"), index_col=0)
else:
    dataset_train = pd.read_csv(os.path.join(base_dir, "dataset_missing", "dataset_train_missing.csv"), index_col=0)
    dataset_test  = pd.read_csv(os.path.join(base_dir, "dataset_missing", "dataset_test_missing.csv"), index_col=0)

x_test_img_path = dataset_test["img"].values
x_test_snd_path = dataset_test["snd"].values
x_test_text = dataset_test["text"].values
y_test = to_categorical(dataset_test["target"].values)

# --- Image
x_test_img = np.zeros((len(x_test_img_path), 299, 299, 3))
for i, p in enumerate(x_test_img_path):
    if p is not None and isinstance(p, str) and os.path.exists(p):
        x_test_img[i] = np.load(p)["img"]
x_test_img = x_test_img.astype("float32") / 255.

# --- Sound
# mel ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ç®—å‡ºï¼ˆpreprocess_audio.py ã¨åŒæ§˜ã®å‡¦ç†ã«åˆã‚ã›ã‚‹ï¼‰
def calculate_melsp(x, n_fft=1024, hop_length=128, sr=44100):
    stft = np.abs(librosa.stft(x, n_fft=n_fft, hop_length=hop_length))**2
    melsp = librosa.feature.melspectrogram(S=stft, sr=sr, n_mels=128)
    melsp_db = librosa.power_to_db(melsp)
    return melsp_db

# æ±ç”¨ãƒ­ãƒ¼ãƒ€: .npz ã®å ´åˆã¯ melsp ã‚’ãã®ã¾ã¾èª­ã¿è¾¼ã¿ã€æ³¢å½¢ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰è¨ˆç®—
def load_melsp_from_path(filepath):
    if filepath is None or not isinstance(filepath, str) or not os.path.exists(filepath):
        return None
    try:
        if filepath.lower().endswith(".npz"):
            d = np.load(filepath)
            # preprocess_audio.py ã§ "melsp" ã‚­ãƒ¼ã§ä¿å­˜ã—ã¦ã„ã‚‹æƒ³å®š
            if "melsp" in d:
                return d["melsp"]
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä»–ã®ã‚­ãƒ¼åã®å¯èƒ½æ€§
            for k in d.files:
                if k.lower().startswith("mel"):
                    return d[k]
            return None
        else:
            # wav ç­‰ã®ã¨ãã¯æ³¢å½¢ã‹ã‚‰ mel ã‚’ä½œæˆ
            x, _sr = librosa.load(filepath, sr=44100)
            return calculate_melsp(x, sr=_sr)
    except Exception as e:
        print(f"[WARN] éŸ³å£°èª­ã¿è¾¼ã¿ã«å¤±æ•—: {filepath} ({e})")
        return None

# å…¨ãƒ†ã‚¹ãƒˆéŸ³å£°ã® mel é•·ã‚’èª¿ã¹ã¦å…¥åŠ›é•·ã‚’æ±ºå®šï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/åˆ‡ã‚Šè©°ã‚ï¼‰
mel_list = []
time_max = 0
freq = 128
for p in x_test_snd_path:
    mel = load_melsp_from_path(p)
    mel_list.append(mel)
    if mel is not None:
        # mel shape: (freq, time)
        if mel.shape[0] != freq:
            # å‘¨æ³¢æ•°æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—/èª¿æ•´ï¼ˆã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å¾Œæ®µã§ã‚¼ãƒ­è©°ã‚ï¼‰
            pass
        time_max = max(time_max, mel.shape[1])

# æ™‚é–“é•·ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®å€¤ã«åˆã‚ã›ã‚‹ï¼‰
if time_max <= 0:
    time_max = 1723

# å›ºå®šé•·ãƒ†ãƒ³ã‚½ãƒ«ã«æ•´å½¢ï¼ˆå³å´ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã€é•·ã„å ´åˆã¯å³å´åˆ‡ã‚Šè½ã¨ã—ï¼‰
x_test_snd = np.zeros((len(x_test_snd_path), freq, time_max), dtype=np.float32)
for i, mel in enumerate(mel_list):
    if mel is None:
        continue
    # å‘¨æ³¢æ•°æ¬¡å…ƒã®æ•´åˆï¼ˆä¸è¶³æ™‚ã¯åˆ‡ã‚Šè©°ã‚ã€è¶…éæ™‚ã¯å…ˆé ­freqãƒãƒ£ãƒãƒ«æ¡ç”¨ï¼‰
    mel_use = mel
    if mel_use.shape[0] != freq:
        if mel_use.shape[0] > freq:
            mel_use = mel_use[:freq, :]
        else:
            tmp = np.zeros((freq, mel_use.shape[1]), dtype=mel_use.dtype)
            tmp[:mel_use.shape[0], :] = mel_use
            mel_use = tmp
    # æ™‚é–“æ¬¡å…ƒã®æ•´åˆ
    T = min(time_max, mel_use.shape[1])
    x_test_snd[i, :, :T] = mel_use[:, :T]

x_test_snd = x_test_snd.reshape(len(x_test_snd), freq, time_max, 1)

# --- Text
txt_length = 200
def convert_text_to_unicode(s, del_rate=0.001):
    return [ord(x) for x in str(s).strip() if random.random() > del_rate] if s != 0 else [0]

def reshape_text(s, max_length=200, del_rate=0.001):
    s_ = convert_text_to_unicode(s, del_rate)
    s_ = s_[:max_length] + [0] * (max_length - len(s_))
    return s_

x_test_text = np.array([reshape_text(t, max_length=txt_length, del_rate=0) for t in x_test_text])

# ====================
# 2. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆç‰¹å¾´æŠ½å‡ºå™¨ï¼‰
# ====================
input_text = Input(shape=(txt_length,), name='input_text')
def clcnn(input_text):
    filter_sizes = (2,3,4,5)
    clx = Embedding(0xffff, 256)(input_text)
    convs = []
    for i, f in enumerate(filter_sizes):
        _x = Conv1D(256, f, strides=(f//2), padding="same")(clx)
        _x = Activation("tanh")(_x)
        _x = GlobalMaxPooling1D()(_x)
        convs.append(_x)
    x = Concatenate()(convs)
    x = Dense(1024, activation="selu", kernel_initializer="lecun_normal")(x)
    x = Dropout(0.1)(x)
    x = Dense(256, activation="selu", kernel_initializer="lecun_normal")(x)
    return x

input_img = Input(shape=(299, 299, 3), name="input_img")
def xception(input_img):
    cnn = Xception(input_tensor=input_img, include_top=False, weights='imagenet')
    x = cnn.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    return x

# éŸ³å£°å…¥åŠ›ã®å½¢çŠ¶ã‚’å‹•çš„ã«è¨­å®š
input_snd = Input(shape=(freq, time_max, 1), name="input_snd")
def snd_cnn(input_snd):
    def cba(x, f, k, s):
        x = Conv2D(f, k, strides=s, padding='same')(x)
        x = BatchNormalization()(x)
        return Activation("relu")(x)
    x_1 = cba(input_snd, 32, (1,8), (1,2))
    x_1 = cba(x_1, 32, (8,1), (2,1))
    x_1 = cba(x_1, 64, (1,8), (1,2))
    x_1 = cba(x_1, 64, (8,1), (2,1))
    x = GlobalAveragePooling2D()(x_1)
    x = Dense(256, activation='relu')(x)
    return x

# ãƒ¢ãƒ‡ãƒ«æ§‹æˆ
clx = clcnn(input_text)
xcp = xception(input_img)
snd = snd_cnn(input_snd)
merged = Concatenate()([clx, xcp, snd])
last_dense = Dense(256, activation='relu', name="last_dense")(merged)
model = Model(inputs=[input_text, input_img, input_snd], outputs=last_dense)

# ====================
# 3. ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
# ====================
features_test = model.predict([x_test_text, x_test_img, x_test_snd])

# ====================
# 4. GMMã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
# ====================
scaler = StandardScaler()
# æ•°å€¤å®‰å®šã®ãŸã‚ float64 ã§æ¨™æº–åŒ–
features_scaled = scaler.fit_transform(features_test.astype(np.float64))

# ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚ˆã‚Šå¤šã„æˆåˆ†æ•°ã¯ä¸å®‰å®šã«ãªã‚‹ãŸã‚æŠ‘åˆ¶
orig_components = y_test.shape[1]
n_samples = features_scaled.shape[0]
n_components = min(orig_components, max(2, n_samples - 1))

# å®‰å®šåŒ–ã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ•ã‚£ãƒƒã‚¿
last_err = None
for cov in ("diag", "spherical"):
    for reg in (1e-4, 1e-3, 1e-2):
        try:
            gmm = GaussianMixture(
                n_components=n_components,
                covariance_type=cov,
                reg_covar=reg,
                random_state=0,
                init_params="kmeans",
                max_iter=200,
            )
            gmm.fit(features_scaled)
            last_err = None
            break
        except Exception as e:
            last_err = e
    if last_err is None:
        break

if last_err is not None:
    raise last_err

gmm_preds = gmm.predict(features_scaled)

y_true = np.argmax(y_test, axis=1)
ari = adjusted_rand_score(y_true, gmm_preds)
print(f"Adjusted Rand Index (ARI): {ari:.4f}")

# ===== å¯è¦–åŒ–: çœŸã®ãƒ©ãƒ™ãƒ« vs äºˆæ¸¬ã‚¯ãƒ©ã‚¹ã‚¿ã®å¯¾å¿œï¼ˆè¡Œæ­£è¦åŒ–ï¼‰ =====
try:
    ct = pd.crosstab(pd.Series(y_true, name="true"), pd.Series(gmm_preds, name="cluster"), normalize="index")
    plt.figure(figsize=(1.2*ct.shape[1]+3, 1.2*ct.shape[0]+3))
    sns.heatmap(ct, annot=True, fmt=".2f", cmap="Blues")
    plt.title(f"Contingency (row-normalized)\nARI={ari:.3f}")
    plt.xlabel("Predicted cluster")
    plt.ylabel("True class")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"[WARN] Heatmap å¯è¦–åŒ–å¤±æ•—: {e}")

# ===== å¯è¦–åŒ–: ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºåˆ†å¸ƒ =====
try:
    counts = np.bincount(gmm_preds, minlength=gmm.n_components)
    plt.figure(figsize=(max(6, 0.8*len(counts)), 4))
    plt.bar(np.arange(len(counts)), counts, color="#4e79a7")
    plt.xlabel("Cluster ID")
    plt.ylabel("Count")
    plt.title("Cluster sizes")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"[WARN] Cluster size å¯è¦–åŒ–å¤±æ•—: {e}")

# ===== Silhouette ã‚¹ã‚³ã‚¢ã¨åˆ†å¸ƒ =====
try:
    sil_overall = silhouette_score(features_scaled, gmm_preds)
    sil_samples = silhouette_samples(features_scaled, gmm_preds)
    print(f"Silhouette score (overall): {sil_overall:.4f}")
    df_sil = pd.DataFrame({"cluster": gmm_preds, "sil": sil_samples})
    plt.figure(figsize=(max(6, 0.8*len(counts)), 4))
    sns.boxplot(data=df_sil, x="cluster", y="sil", color="#59a14f")
    plt.axhline(sil_overall, ls="--", c="red", lw=1, label=f"overall={sil_overall:.3f}")
    plt.legend()
    plt.title("Silhouette by cluster")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print(f"[WARN] Silhouette å¯è¦–åŒ–å¤±æ•—: {e}")

# ===== t-SNE 2D å¯è¦–åŒ–ï¼ˆé‡ã„å ´åˆã‚ã‚Šï¼‰ =====
try:
    n = len(features_scaled)
    if n >= 5:
        perplexity = max(5, min(30, n // 10))
        tsne = TSNE(n_components=2, init="pca", random_state=0, perplexity=perplexity)
        emb = tsne.fit_transform(features_scaled)

        plt.figure(figsize=(6,5))
        plt.scatter(emb[:,0], emb[:,1], c=gmm_preds, cmap="tab20", s=18, alpha=0.9)
        plt.title("t-SNE colored by predicted cluster")
        plt.xticks([]); plt.yticks([])
        plt.tight_layout(); plt.show()

        plt.figure(figsize=(6,5))
        plt.scatter(emb[:,0], emb[:,1], c=y_true, cmap="tab20", s=18, alpha=0.9)
        plt.title("t-SNE colored by true class")
        plt.xticks([]); plt.yticks([])
        plt.tight_layout(); plt.show()
except Exception as e:
    print(f"[WARN] t-SNE å¯è¦–åŒ–å¤±æ•—: {e}")

# ====================
# 5. çµæœã®ç¢ºèª
# ====================

n_clusters = gmm.n_components

def display_sample_for_cluster(cluster_id):
    # cluster ã«å±ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    idxs = np.where(gmm_preds == cluster_id)[0]
    if len(idxs) == 0:
        print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã«å±ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãªã—")
        return
    # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ
    sel = random.choice(idxs)
    print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} â€” é¸æŠã‚µãƒ³ãƒ—ãƒ« index: {sel}")
    
    # ç”»åƒè¡¨ç¤º
    img_path = dataset_test.iloc[sel]["img"]
    if img_path is not None and os.path.exists(img_path):
        arr = np.load(img_path)["img"]  # .npz ã«ä¿å­˜ã—ã¦ã„ã‚‹å‰æ
        # æ­£è¦åŒ–ãªã©é€†å¤‰æ›ãŒã‚ã‚Œã°ã“ã“ã§ã‚„ã‚‹
        plt.figure(figsize=(4,4))
        plt.imshow(arr.astype(np.uint8))
        plt.axis("off")
        plt.title(f"ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id} ã®ç”»åƒ (idx={sel})")
        plt.show()
    else:
        print("ç”»åƒãƒ‡ãƒ¼ã‚¿ãªã—")
    
    # éŸ³å£°: .wav ãªã‚‰å†ç”Ÿã€.npz ãªã‚‰ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‚’å¯è¦–åŒ–
    snd_path = dataset_test.iloc[sel]["snd"]
    if snd_path is not None and os.path.exists(snd_path):
        if snd_path.lower().endswith(".npz"):
            try:
                mel = np.load(snd_path)["melsp"]
                plt.figure(figsize=(5, 3))
                plt.imshow(mel, origin="lower", aspect="auto", cmap="magma")
                plt.colorbar(label="dB")
                plt.title("Mel spectrogram (from .npz)")
                plt.tight_layout()
                plt.show()
            except Exception as e:
                print(f"éŸ³å£°ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ è¡¨ç¤ºå¤±æ•—: {e}")
        else:
            display_audio = ipd.Audio(snd_path)
            print("ğŸ”Š éŸ³å£°å†ç”Ÿï¼š")
            ipd.display(display_audio)
    else:
        print("éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãªã—")
    
    # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
    text = dataset_test.iloc[sel]["text"]
    print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ:", repr(text))
    print()  # æ”¹è¡Œ

# å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã—ã¦è¡¨ç¤ºã™ã‚‹
for c in range(n_clusters):
    display_sample_for_cluster(c)
