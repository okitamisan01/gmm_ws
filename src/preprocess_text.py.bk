# python3 src/preprocess_text.py --auto --max-pages 30 --chunk-chars 600 --lang ja
# Wikipediaで生成させようとしていたが、saucydogなどの情報が入るためだめ

import argparse
import json
import os
import re
import time
from typing import Dict, List
from urllib.parse import quote

try:
    import requests  # lightweight HTTP client
except Exception:  # pragma: no cover
    requests = None

# Add a proper User-Agent per Wikipedia API etiquette
HEADERS = {
    "User-Agent": "gmm-ws-textbuilder/0.1 (Linux; +https://www.mediawiki.org/wiki/API:Etiquette)",
    "Accept": "application/json",
}

# Chosen category mapping (Caltech256 class ID -> ESC-50 target ID)
# Keep in sync with preprocessing/making_category.py selection
chosen_oc_esc = {
    58: 30,
    102: 40,
    239: 35,
    245: 16,
    113: 14,
    170: 10,
    89: 1,
    73: 48,
    251: 47,
    56: 0,
    80: 4,
}

# Japanese title overrides for Caltech256 class names -> canonical ja.wikipedia titles
# 追加：曖昧/英語名に対する日本語タイトルの手動オーバーライド
JAPANESE_TITLE_OVERRIDES = {
    "dog": "イヌ",
    "frog": "カエル",
    "helicopter": "ヘリコプター",
    "wind": "風",
    "hummingbird": "ハチドリ",
    "goose": "ガン (鳥)",
    "fireworks": "花火",
    "airplane": "飛行機",
    "washing-machine": "洗濯機",
    "windmill": "風車",
    "doorknob": "ドアノブ",
    "rainbow": "虹",
}



def repo_root() -> str:
    """Return repository root path (one level up from this file)."""
    here = os.path.dirname(os.path.abspath(__file__))
    return os.path.dirname(here)


def sanitize_name(name: str) -> str:
    """Make a filesystem-friendly name."""
    s = name.strip().lower()
    s = re.sub(r"\s+", "-", s)
    s = re.sub(r"[^a-z0-9._\-]", "", s)
    return s


def load_oc256_classes(base_dir: str) -> Dict[int, str]:
    """Scan Caltech256 folder to build mapping {class_id: class_name}."""
    oc256_dir = os.path.join(base_dir, "data_raw", "Caltech256", "256_ObjectCategories")
    if not os.path.isdir(oc256_dir):
        raise FileNotFoundError(f"Not found: {oc256_dir}")
    result: Dict[int, str] = {}
    for entry in os.listdir(oc256_dir):
        # Expect names like '056.dog'
        try:
            label_str, name = entry.split(".", 1)
            cid = int(label_str)
            result[cid] = name
        except Exception:
            # Skip unexpected entries
            continue
    if not result:
        raise RuntimeError("Caltech256 classes not found. Check dataset layout.")
    return result


def get_text_root(base_dir: str) -> str:
    return os.path.join(base_dir, "data_processed", "text")


def init_text_folders(base_dir: str) -> None:
    """Create data_processed/text/<cid>-<name>/ folders and a placeholder file if empty."""
    oc_classes = load_oc256_classes(base_dir)
    root = get_text_root(base_dir)
    os.makedirs(root, exist_ok=True)

    for cid in chosen_oc_esc.keys():
        cname = oc_classes.get(cid, f"class-{cid}")
        folder = os.path.join(root, f"{cid}-{sanitize_name(cname)}")
        os.makedirs(folder, exist_ok=True)

        # Drop a placeholder if the folder is empty
        if not any(os.scandir(folder)):
            placeholder = os.path.join(folder, "00_placeholder.txt")
            with open(placeholder, "w", encoding="utf-8") as f:
                f.write(
                    "このファイルはプレースホルダーです。\n"
                    "同じフォルダーに Wikipedia（日本語）からコピペしたテキストを .txt または .md で保存してください。\n"
                    "1ファイル=1サンプルとして扱います。\n"
                )

    # Write a manifest for convenience
    manifest_path = os.path.join(root, "manifest.json")
    manifest = {
        "text_root": root,
        "categories": {
            str(cid): {
                "name": oc_classes.get(cid, f"class-{cid}"),
                "folder": f"{cid}-{sanitize_name(oc_classes.get(cid, f'class-{cid}'))}",
            }
            for cid in chosen_oc_esc.keys()
        },
    }
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)


def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read().strip()


def build_oc_txt_dict(base_dir: str) -> Dict[int, List[str]]:
    """Read .txt/.md under data_processed/text/<cid>-<name>/ and build {cid: [texts...]}."""
    root = get_text_root(base_dir)
    if not os.path.isdir(root):
        raise FileNotFoundError(
            f"Text root not found: {root}. Run with --init and place text files first."
        )

    oc_txt_dict: Dict[int, List[str]] = {}
    for entry in os.scandir(root):
        if not entry.is_dir():
            continue
        # Folder format: <cid>-<name>
        try:
            cid_str = entry.name.split("-", 1)[0]
            cid = int(cid_str)
        except Exception:
            continue

        texts: List[str] = []
        for fentry in os.scandir(entry.path):
            if not fentry.is_file():
                continue
            if not (fentry.name.endswith(".txt") or fentry.name.endswith(".md")):
                continue
            if fentry.name.upper().startswith("ATTRIBUTION"):
                continue
            content = read_text_file(fentry.path)
            if content:
                texts.append(content)
        if texts:
            oc_txt_dict[cid] = texts

    if not oc_txt_dict:
        raise RuntimeError(
            "No text samples found. Place .txt/.md files into each category folder and retry."
        )

    # Save as JSON (UTF-8, keep Japanese as-is)
    out_json = os.path.join(root, "oc_txt_dict.json")
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump({str(k): v for k, v in oc_txt_dict.items()}, f, ensure_ascii=False, indent=2)

    return oc_txt_dict


def _wiki_search_titles(term: str, lang: str = "ja", limit: int = 5) -> List[str]:
    if requests is None:
        raise RuntimeError("requests が見つかりません。requirements.txt に requests を追加してください。")
    url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "list": "search",
        "srsearch": term,
        "format": "json",
        "srlimit": limit,
        "utf8": 1,
        "srnamespace": 0,  # 記事本文のみ
        "srqiprofile": "classic_noboostlinks",
    }
    r = requests.get(url, params=params, headers=HEADERS, timeout=20)
    r.raise_for_status()
    data = r.json()
    hits = data.get("query", {}).get("search", [])
    titles = [h.get("title") for h in hits if h.get("title")]
    return titles


def _wiki_fetch_summary(title: str, lang: str = "ja") -> Dict[str, str]:
    if requests is None:
        raise RuntimeError("requests が見つかりません。requirements.txt に requests を追加してください。")
    api = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{quote(title)}"
    r = requests.get(api, headers=HEADERS, timeout=20)
    if r.status_code == 404:
        return {}
    r.raise_for_status()
    js = r.json()
    # disambiguation/redirect は除外
    if js.get("type") in ("disambiguation", "redirect"):
        return {}
    extract = js.get("extract") or ""
    # タイトル末尾の（…）は原則除外（全角）
    ttl = js.get("title", title) or title
    if re.search(r"（.+）$", ttl):
        return {}
    url = (
        js.get("content_urls", {})
        .get("desktop", {})
        .get("page", f"https://{lang}.wikipedia.org/wiki/{quote(title)}")
    )
    desc = js.get("description") or ""
    return {"title": ttl, "url": url, "extract": extract, "description": desc}


def _wiki_langlink(title: str, from_lang: str = "en", to_lang: str = "ja") -> str:
    """Resolve an interlanguage link from from_lang title to to_lang title. Returns '' if not found."""
    if requests is None:
        raise RuntimeError("requests が見つかりません。requirements.txt に requests を追加してください。")
    url = f"https://{from_lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "prop": "langlinks",
        "titles": title,
        "lllang": to_lang,
        "format": "json",
        "utf8": 1,
        "lllimit": 50,
    }
    r = requests.get(url, params=params, headers=HEADERS, timeout=20)
    r.raise_for_status()
    data = r.json()
    pages = data.get("query", {}).get("pages", {})
    for _, page in pages.items():
        lls = page.get("langlinks") or []
        for ll in lls:
            if ll.get("lang") == to_lang and ll.get(""*0 + "*", ll.get("title")):
                # MediaWiki returns the translated title in '*' key; 'title' for some clients
                return ll.get("*", ll.get("title", ""))
    return ""


def _generate_query_variants(name: str) -> List[str]:
    base = name.replace("_", " ").replace(".", " ")
    base = re.sub(r"-\d+$", "", base).replace("-", " ").strip()
    variants = []
    # ① override があれば最優先
    if name in JAPANESE_TITLE_OVERRIDES:
        variants.append(JAPANESE_TITLE_OVERRIDES[name])
    if base in JAPANESE_TITLE_OVERRIDES:
        variants.append(JAPANESE_TITLE_OVERRIDES[base])
    # ② 通常化した候補
    variants += [name, base]
    # ③ 分野ヒントを追加（あいまいさ回避を避けるため）
    HINTS = {
        "dog": "動物", "frog": "両生類", "hummingbird":"鳥",
        "helicopter":"航空機", "airplanes":"航空機",
        "washing machine":"家電", "windmill":"機械", "rainbow":"気象",
        "wind":"気象", "goose":"鳥", "fireworks":"行事",
        "doorknob":"建築金物",
    }
    key = base.lower()
    for k,v in HINTS.items():
        if k in key:
            # override があればそれ、なければ base を使ってヒント付与
            jp = JAPANESE_TITLE_OVERRIDES.get(base, base)
            variants.append(f"{jp} {v}")
            break
    # 重複除去
    seen, uniq = set(), []
    for v in variants:
        if v and v not in seen:
            uniq.append(v); seen.add(v)
    return uniq



def _split_into_chunks(text: str, max_chars: int = 600) -> List[str]:
    paras = [p.strip() for p in text.split("\n") if p.strip()]
    chunks: List[str] = []
    buf = ""
    for p in paras:
        if len(buf) + len(p) + 1 <= max_chars:
            buf = (buf + "\n" + p).strip()
        else:
            if buf:
                chunks.append(buf)
            if len(p) <= max_chars:
                buf = p
            else:
                # Hard wrap very long paragraph
                start = 0
                while start < len(p):
                    chunks.append(p[start : start + max_chars])
                    start += max_chars
                buf = ""
    if buf:
        chunks.append(buf)
    return chunks


def auto_collect_from_wikipedia(base_dir: str, max_pages: int = 5, chunk_chars: int = 600, lang: str = "ja", delay_sec: float = 0.2) -> None:
    """For each chosen category, search ja.wikipedia and save summary text samples.

    Creates files under data_processed/text/<cid>-<name>/ and writes ATTRIBUTION.txt with sources.
    """
    oc_classes = load_oc256_classes(base_dir)
    root = get_text_root(base_dir)
    os.makedirs(root, exist_ok=True)

    for cid in chosen_oc_esc.keys():
        cname = oc_classes.get(cid, f"class-{cid}")
        folder = os.path.join(root, f"{cid}-{sanitize_name(cname)}")
        os.makedirs(folder, exist_ok=True)

        # Try multiple query variants in Japanese first
        titles: List[str] = []
        for q in _generate_query_variants(cname):
            try:
                titles = _wiki_search_titles(q, lang=lang, limit=max_pages)
                if titles:
                    break
            except Exception as e:
                print(f"[WARN] Wikipedia検索失敗 cid={cid} name={cname} q='{q}': {e}")
        # Fallback: search in English then resolve ja title via langlinks
        if not titles:
            try:
                en_titles = _wiki_search_titles(_generate_query_variants(cname)[-1], lang="en", limit=max_pages)
                titles = []
                for et in en_titles:
                    ja_title = _wiki_langlink(et, from_lang="en", to_lang=lang) or ""
                    if ja_title:
                        titles.append(ja_title)
            except Exception as e:
                print(f"[WARN] 英語検索/言語リンク解決失敗 cid={cid} name={cname}: {e}")

        sources = []
        saved = 0
        for t in titles:
            try:
                meta = _wiki_fetch_summary(t, lang=lang)
            except Exception as e:
                print(f"[WARN] 要約取得失敗 '{t}': {e}")
                continue
            if not meta or not meta.get("extract"):
                continue
            # 明らかに無関係な説明は除外（例：バンド・楽曲等）
            desc = (meta.get("description") or "").lower()
            if any(kw in desc for kw in ["バンド", "ロックバンド", "楽曲", "シングル", "漫画", "アニメ", "テレビ", "企業"]):
                continue
            chunks = _split_into_chunks(meta["extract"], max_chars=chunk_chars)
            title_slug = sanitize_name(meta["title"]) or "page"
            for i, ch in enumerate(chunks, start=1):
                fname = os.path.join(folder, f"{str(saved+1).zfill(3)}_{title_slug}_{i}.txt")
                with open(fname, "w", encoding="utf-8") as f:
                    f.write(ch)
                saved += 1
            sources.append({"title": meta["title"], "url": meta["url"]})
            time.sleep(delay_sec)

        if sources:
            attr = os.path.join(folder, "ATTRIBUTION.txt")
            with open(attr, "w", encoding="utf-8") as f:
                f.write("Source: Wikipedia（日本語）\n")
                f.write("License: CC BY-SA 3.0\n")
                f.write("各サンプルは上記出典に基づく要約テキストです。再配布時はライセンスと出典を表示してください。\n\n")
                for s in sources:
                    f.write(f"- {s['title']}: {s['url']}\n")

        if saved:
            placeholder = os.path.join(folder, "00_placeholder.txt")
            if os.path.exists(placeholder):
                try:
                    os.remove(placeholder)
                except OSError:
                    pass

        print(f"[INFO] {cid}-{cname}: {saved} サンプル自動生成")


def main():
    parser = argparse.ArgumentParser(description="Prepare and build text data per category.")
    parser.add_argument("--init", action="store_true", help="Create folders for text data.")
    parser.add_argument(
        "--build",
        action="store_true",
        help="Build oc_txt_dict.json by reading .txt/.md files.",
    )
    parser.add_argument(
        "--auto", action="store_true", help="Use Wikipedia API to auto-collect text samples."
    )
    parser.add_argument("--max-pages", type=int, default=5, help="Max search pages per category.")
    parser.add_argument("--chunk-chars", type=int, default=600, help="Max characters per sample chunk.")
    parser.add_argument("--lang", type=str, default="ja", help="Wikipedia language code (default: ja)")
    args = parser.parse_args()

    base = repo_root()

    if args.init:
        init_text_folders(base)
        print("Initialized data_processed/text structure and manifest.json.")

    if getattr(args, "auto", False):
        auto_collect_from_wikipedia(base, max_pages=args.max_pages, chunk_chars=args.chunk_chars, lang=args.lang)

    if args.build:
        oc_txt = build_oc_txt_dict(base)
        counts = {k: len(v) for k, v in oc_txt.items()}
        print("Built oc_txt_dict.json with counts:")
        for k in sorted(counts.keys()):
            print(f"  {k}: {counts[k]}")

    if not args.init and not getattr(args, "auto", False) and not args.build:
        print(
            "Nothing to do. Use --init to scaffold folders, --auto to collect from Wikipedia, and --build to generate oc_txt_dict.json."
        )


if __name__ == "__main__":
    main()